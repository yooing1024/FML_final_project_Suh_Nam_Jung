{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da45a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbef0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skmultilearn.model_selection import iterative_train_test_split\n",
    "# t_train, y_train, t_test, y_test = iterative_train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0362898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Load and split\n",
    "\n",
    "# 96 eyes, 49 OCT images, 2 visits, 16 biomarkers (binary)\n",
    "# 96 eyes, 49 OCT images, 2 visits, 496 x 504 OCT images (grayscale)\n",
    "scan_N = 9408\n",
    "oct_N = 49\n",
    "eye_N = 96\n",
    "sh = [496, 504]\n",
    "\n",
    "csv_file = '~/scratch/OLIVES/OLIVES/Biomarker_Clinical_Data_Images_Updated.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "col_names = data.columns\n",
    "file_paths = data['Path (Trial/Arm/Folder/Visit/Eye/Image Name)'].values #[9408,]\n",
    "file_paths = file_paths.reshape([eye_N,2*oct_N])\n",
    "bio_markers = data[col_names[2:18]].values\n",
    "bio_markers = bio_markers.reshape([eye_N,2*oct_N,-1])\n",
    "\n",
    "clin_data = data[col_names[19:21]].values\n",
    "clin_data = clin_data.reshape([eye_N,2*oct_N,-1])\n",
    "\n",
    "home_dir = '/home/hice1/hsuh45/scratch/OLIVES/OLIVES/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8e07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with Nan and identify the rows (get rid of them after Data split)\n",
    "rows_with_nan = data[data.isna().any(axis=1)]\n",
    "# data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f43b2a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 98) (19, 98) (20, 98)\n"
     ]
    }
   ],
   "source": [
    "# DeiT preprocessing\n",
    "# transform_deit = transforms.Compose([\n",
    "#     transforms.Resize((512,512)),                # Resize to square dimensions\n",
    "#     transforms.ToTensor(),                        # Convert to tensor\n",
    "#     transforms.Normalize(mean=[0.5], std=[0.5])   # Normalize (adjust mean/std for grayscale)\n",
    "# ])\n",
    "# SwinT preprocessing\n",
    "# resize to square + fit the input size (due to small dataset)\n",
    "\n",
    "\n",
    "########## Preprocessing ###########################\n",
    "model_name = 'deit_base_patch16_224'\n",
    "input_dim = 224\n",
    "transform_swin = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels\n",
    "    transforms.CenterCrop(496),\n",
    "    transforms.Resize((input_dim,input_dim)),                # 224x224 or 384x384\n",
    "    transforms.ToTensor(),                        # Convert to tensor\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # Normalize with ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#####################################################\n",
    "\n",
    "# Create DataLoaders with the preprocessed data\n",
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = Image.open(home_dir + self.file_paths[index][0]).convert(\"L\")\n",
    "        label = self.labels[index]  # Shape: [sample N, bio_marker_N]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "########## Get rid of Nan values in bio_markers ###########\n",
    "nan_rows = np.isnan(bio_markers).any(axis=2)  # Shape (96, 98), True where NaNs are present\n",
    "print(np.sum(nan_rows))\n",
    "# Replace NaN rows with alternating 1 and -1\n",
    "alternating_row = np.tile([1, -1], 8)  # Alternating 1 and -1, length 16 (biomarker dimension)\n",
    "bio_markers[nan_rows] = alternating_row\n",
    "\n",
    "###### Split dataset into train/val/test (based on number of positives)\n",
    "\n",
    "\n",
    "pos_count = np.sum(bio_markers,axis=(1,2)) # pos label count per eye\n",
    "bins = [0,200,300,400,np.inf]\n",
    "print(pos_count)\n",
    "stratify_bins = np.digitize(pos_count, bins)\n",
    "print(stratify_bins)\n",
    "train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "    file_paths, bio_markers, test_size=0.2,\n",
    "    stratify = stratify_bins,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pos_count = np.sum(train_val_labels,axis=(1,2)) # pos label count per eye\n",
    "bins = [0,200,300,400,np.inf]\n",
    "stratify_bins = np.digitize(pos_count, bins)\n",
    "print(pos_count)\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_val_files, train_val_labels, test_size=0.25, \n",
    "    stratify = stratify_bins,\n",
    "    random_state=42\n",
    ")\n",
    "print(train_files.shape, val_files.shape, test_files.shape)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "\n",
    "# Eye-wise -> scan-wise\n",
    "train_files = train_files.reshape([-1,1])\n",
    "val_files = val_files.reshape([-1,1])\n",
    "test_files = test_files.reshape([-1,1])\n",
    "\n",
    "train_labels = train_labels.reshape([-1,16])\n",
    "val_labels = val_labels.reshape([-1,16])\n",
    "test_labels = test_labels.reshape([-1,16])\n",
    "\n",
    "######## Get rid of data points with Nan values #########\n",
    "def remove_marked_rows(files, labels):\n",
    "    # Identify rows where any column has -1 (our marker for NaNs)\n",
    "    marked_rows = (labels == -1).all(axis=1) # shape(N,16)\n",
    "    filtered_labels = labels[~marked_rows]\n",
    "    filtered_files = files[~marked_rows]\n",
    "    return filtered_files, filtered_labels\n",
    "\n",
    "# print(train_labels)\n",
    "train_files, train_labels = remove_marked_rows(train_files, train_labels)\n",
    "val_files, val_labels = remove_marked_rows(val_files, val_labels)\n",
    "test_files, test_labels = remove_marked_rows(test_files, test_labels)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "train_dataset = OCTDataset(train_files, train_labels, transform=transform_swin)\n",
    "val_dataset = OCTDataset(val_files, val_labels, transform=transform_swin)\n",
    "test_dataset = OCTDataset(test_files, test_labels, transform=transform_swin)\n",
    "\n",
    "# Make DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671257f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(home_dir + file_paths[0][0])\n",
    "np.array(img).shape\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6acbecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  6.,  8.,  9., 11., 10.,  9., 13.,  4.,  4.]),\n",
       " array([ 96. , 131.6, 167.2, 202.8, 238.4, 274. , 309.6, 345.2, 380.8,\n",
       "        416.4, 452. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check pos_sum distribution for stratification label\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(np.sum(train_val_labels, axis=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e32f4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5586, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb742cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 98, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(test_labels).sum()\n",
    "bio_markers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1de52cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.007287    0.02192327  0.00274041  0.01033881  0.22732935  0.10812157\n",
      "  0.20204285  0.0297708   0.12176134 -0.00031141  0.1114848   0.14798206\n",
      "  0.00772297 -0.00043597  0.0013702   0.00087195]\n",
      "[0.00923077 0.0225     0.         0.00923077 0.24961538 0.11442308\n",
      " 0.18403846 0.0325     0.09365385 0.00057692 0.10557692 0.16153846\n",
      " 0.01134615 0.00076923 0.         0.005     ]\n",
      "[ 0.02094522  0.06301468  0.00787683  0.02971715  0.65341926  0.31077694\n",
      "  0.58073756  0.08557107  0.3499821  -0.00089509  0.32044397  0.42534909\n",
      "  0.02219835 -0.00125313  0.00393842  0.00250627]\n",
      "[0.02577873 0.06283566 0.         0.02577873 0.69709989 0.31954887\n",
      " 0.51396348 0.09076262 0.26154672 0.00161117 0.29484425 0.45112782\n",
      " 0.03168636 0.00214823 0.         0.01396348]\n",
      "[ 1.89200680e-02  6.29251701e-02  4.67687075e-03  3.83715986e-02\n",
      "  6.74957483e-01  3.15901361e-01  5.55697279e-01  8.45025510e-02\n",
      "  3.02721088e-01 -2.12585034e-04  3.20471939e-01  4.33142007e-01\n",
      "  2.60416667e-02 -2.12585034e-04  2.33843537e-03  6.80272109e-03]\n"
     ]
    }
   ],
   "source": [
    "# Quick analysis of biomarker distribution\n",
    "\n",
    "print(np.sum(train_labels, axis=(0)) / np.sum(train_labels))\n",
    "print(np.sum(val_labels, axis=(0)) / np.sum(val_labels))\n",
    "\n",
    "print(np.sum(train_labels, axis=(0))/train_labels.shape[0])\n",
    "print(np.sum(val_labels, axis=(0))/val_labels.shape[0])\n",
    "# discrepancy between train vs val class distribution (not too severe (?))\n",
    "\n",
    "print(np.sum(bio_markers,axis=(0,1)) / (bio_markers.shape[0]*bio_markers.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd5bf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install timm\n",
    "torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65d5af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted BCE for multi-label imbalanced (pos vs. neg) data\n",
    "\n",
    "train_pos_weights = train_labels.shape[0] / (2* np.sum(train_labels,axis=0))\n",
    "# val_pos_weights = val_labels.shape[0] / (2* np.sum(val_labels,axis=0))\n",
    "# test_pos_weights = test_labels.shape[0] / (2* np.sum(test_labels,axis=0))\n",
    "\n",
    "class WeightedBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, pos_weights):\n",
    "        \"\"\"\n",
    "        pos_weights: Tensor of shape (num_biomarkers,) containing weights for positive labels.\n",
    "        \"\"\"\n",
    "        super(WeightedBinaryCrossEntropyLoss, self).__init__()\n",
    "        self.pos_weights = pos_weights\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        logits: Predicted logits from the model, shape (batch_size, num_biomarkers).\n",
    "        targets: Ground truth binary labels, shape (batch_size, num_biomarkers).\n",
    "        \"\"\"\n",
    "        loss = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)  # Compute BCE loss\n",
    "        weighted_loss = loss * self.pos_weights  # Apply positive weights\n",
    "        return weighted_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfec18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss for multi-label classification.\n",
    "\n",
    "        Parameters:\n",
    "        - gamma (float): Focusing parameter that reduces the loss for well-classified samples (default: 2.0).\n",
    "        - alpha (float or Tensor): Balancing factor to address class imbalance (default: None).\n",
    "          If a tensor is provided, it should be of shape (num_classes,).\n",
    "        - reduction (str): Specifies the reduction to apply to the output: 'none', 'mean', 'sum' (default: 'mean').\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Compute Focal Loss.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Tensor): Predicted logits of shape (batch_size, num_classes).\n",
    "        - targets (Tensor): Ground truth labels of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "        - loss (Tensor): Calculated focal loss.\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # Binary cross-entropy loss\n",
    "        bce_loss = F.binary_cross_entropy(probs, targets, reduction='none')\n",
    "        \n",
    "        # Compute the modulating factor (1 - p_t)^gamma\n",
    "        pt = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_factor = (1 - pt) ** self.gamma\n",
    "\n",
    "        # Apply class balancing factor alpha if provided\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (float, int)):\n",
    "                alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            elif isinstance(self.alpha, torch.Tensor):\n",
    "                alpha_factor = self.alpha.unsqueeze(0) * targets + (1 - self.alpha).unsqueeze(0) * (1 - targets)\n",
    "            else:\n",
    "                raise ValueError(\"Alpha must be a float, int, or torch.Tensor.\")\n",
    "            focal_loss = alpha_factor * focal_factor * bce_loss\n",
    "        else:\n",
    "            focal_loss = focal_factor * bce_loss\n",
    "\n",
    "        # Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Initialize Focal Loss (Example)\n",
    "# 1. gamma (Focusing Parameter):\n",
    "# Controls the strength of the focusing effect.\n",
    "# Higher values put more focus on hard-to-classify samples.\n",
    "\n",
    "# 2. alpha (Class Balancing Factor):\n",
    "# Helps address class imbalance.\n",
    "# If alpha is a scalar, it applies the same balancing for all classes.\n",
    "# If alpha is a tensor, it applies per-class balancing.\n",
    "\n",
    "# 3. reduction:\n",
    "# 'mean': Average loss across the batch.\n",
    "# 'sum': Sum of the loss across the batch.\n",
    "# 'none': No reduction is applied; returns loss for each sample.\n",
    "\n",
    "focal_loss = FocalLoss(gamma=2.0, alpha=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4852fb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=1000, bias=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.3, inplace=False)\n",
      "  (3): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (4): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     19\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;241m512\u001b[39m),\n\u001b[1;32m     20\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     nn\u001b[38;5;241m.\u001b[39mSigmoid()  \u001b[38;5;66;03m# Multi-label classification (probabilities for each class)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mhead)\n\u001b[0;32m---> 27\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Freeze Vision Encoder layers if needed\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import timm \n",
    "import tqdm\n",
    "\n",
    "\n",
    "# Load model (model_name, )\n",
    "# model_name = 'deit_base_patch16_224'\n",
    "# input_dim = 224\n",
    "print(f'model name {model_name}, input size {input_dim}')\n",
    "model = timm.create_model(model_name, pretrained=True) \n",
    "\n",
    "\n",
    "print(model.head)\n",
    "###### Parameters ######\n",
    "lr = 1e-4\n",
    "num_classes = 16\n",
    "epochs = 10\n",
    "########################\n",
    "\n",
    "# Modify the classifier head for multi-class output\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(model.head.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, num_classes),  # 16 biomarkers\n",
    "    nn.Sigmoid()  # Multi-label classification (probabilities for each class)\n",
    ")\n",
    "\n",
    "print(model.head)\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Freeze Vision Encoder layers if needed\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.head.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "##########Loss ##################\n",
    "\n",
    "criterion = torch.nn.BCELoss()  # Binary Cross-Entropy Loss for multi-label classification\n",
    "# criterion = WeightedBinaryCrossEntropyLoss(train_pos_weights)\n",
    "# criterion = FocalLoss(gamma=2.0, alpha=0.25)\n",
    "#################################\n",
    "\n",
    "# Training and validation\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader):\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "#         print(images.shape, labels.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "#         print(outputs.shape, labels.shape, images.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm.tqdm(val_loader):\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            all_outputs.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    return (running_loss / len(val_loader)), all_outputs, all_labels\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_outputs, val_labels = validate_one_epoch(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "# Test the model\n",
    "# def test_model(model, test_loader, device):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     threshold = 0.5\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to('cuda'), labels.to('cuda')\n",
    "#             outputs = model(images)\n",
    "#             predictions = (outputs > threshold).float()  # Threshold at 0.5 for binary decisions\n",
    "#             correct += (predictions == labels).sum().item()\n",
    "#             total += labels.numel()\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Evaluate on test set\n",
    "# test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize biomarker-wise threshold for validation on eval metrics\n",
    "\n",
    "def optimal_thresholds(model_outputs, labels, metric='f1'):\n",
    "    \"\"\"\n",
    "    Calculate optimal thresholds for each biomarker.\n",
    "    \n",
    "    Parameters:\n",
    "        model_outputs (ndarray): Model predictions, shape (N, 16) where N is the number of samples.\n",
    "        labels (ndarray): True binary labels, shape (N, 16).\n",
    "        metric (str): Metric to optimize. Options: 'f1', 'auc'.\n",
    "\n",
    "    Returns:\n",
    "        thresholds (list): Optimal threshold for each biomarker.\n",
    "        scores (list): Corresponding best scores for each biomarker.\n",
    "    \"\"\"\n",
    "    num_biomarkers = model_outputs.shape[1]\n",
    "    thresholds = []\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(num_biomarkers):\n",
    "        best_threshold = 0.0\n",
    "        best_score = 0.0\n",
    "        \n",
    "        # Thresholds to search\n",
    "        thresholds_range = np.linspace(0, 1, 100)\n",
    "        \n",
    "        for threshold in thresholds_range:\n",
    "            preds = (model_outputs[:, i] >= threshold).astype(int)\n",
    "            \n",
    "            if metric == 'f1':\n",
    "                score = sklearn.metrics.f1_score(labels[:, i], preds)\n",
    "            elif metric == 'auc':\n",
    "                # AUC does not depend on a threshold\n",
    "                score = sklearn.metrics.roc_auc_score(labels[:, i], model_outputs[:, i])\n",
    "                best_threshold = None  # No threshold needed for AUC\n",
    "                break\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported metric. Use 'f1' or 'auc'.\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        thresholds.append(best_threshold)\n",
    "        scores.append(best_score)\n",
    "    \n",
    "    return thresholds, scores\n",
    "\n",
    "f1_th, f1_scores = optimal_thresholds(val_outputs, val_labels, metric='f1')\n",
    "auc_th, auc_scores = optimal_thresholds(val_outputs, val_labels, metric='auc')\n",
    "print(f'F1 threshold {f1_th}, F1 validation scores {f1_scores})\n",
    "print(f'AUC threshold {auc_th}, AUC validation scores {auc_scores})\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f66e4c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished iteration\n",
      "[0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "0.0 0.5\n",
      "0.0 0.5\n",
      "0.0 0.5\n",
      "0.08571428571428572 0.5234961383661482\n",
      "0.8512679917751885 0.7004670206640651\n",
      "0.3918799646954987 0.5852806999180057\n",
      "0.6673407482305359 0.6723738922057378\n",
      "0.0213903743315508 0.5041510611735331\n",
      "0.3088512241054614 0.5859964787220338\n",
      "0.0 0.5\n",
      "0.8469860896445132 0.8742755553127461\n",
      "0.8386363636363636 0.8520721444226941\n",
      "0.31746031746031744 0.5943396226415094\n",
      "0.0 0.5\n",
      "0.0 nan\n",
      "0.0 0.5\n"
     ]
    }
   ],
   "source": [
    "# Get output, save output, get F1 scores and AUC \n",
    "import sklearn\n",
    "test_shape = test_labels.shape\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "############################################\n",
    "threshold = f1_th # F1 vs. AUC threshold\n",
    "threshold = threshold.reshape(1,16)\n",
    "############################################\n",
    "model_output = np.zeros(test_shape)\n",
    "target = np.zeros(test_shape)\n",
    "pred = np.zeros(test_shape)\n",
    "batch_size = 32\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (images, labels) in enumerate(test_loader):\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = model(images)\n",
    "        model_output[batch*batch_size: (batch+1)*batch_size] = outputs\n",
    "        pred[batch*batch_size: (batch+1)*batch_size] = (outputs > threshold).float().cpu()  # Threshold at 0.5 for binary decisions\n",
    "        target[batch*batch_size: (batch+1)*batch_size] = labels.cpu()\n",
    "\n",
    "print(\"Finished iteration\")\n",
    "# print(pred[0])\n",
    "# print(target[0])\n",
    "for i in range(num_classes):\n",
    "    f1 = sklearn.metrics.f1_score(target[:,i], pred[:,i], zero_division =0)\n",
    "    try:\n",
    "        auc = sklearn.metrics.roc_auc_score(target[:,i],pred[:,i])\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "    print(f1, auc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
