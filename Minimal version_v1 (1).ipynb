{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da45a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbef0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skmultilearn.model_selection import iterative_train_test_split\n",
    "# t_train, y_train, t_test, y_test = iterative_train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0362898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Load and split\n",
    "\n",
    "# 96 eyes, 49 OCT images, 2 visits, 16 biomarkers (binary)\n",
    "# 96 eyes, 49 OCT images, 2 visits, 496 x 504 OCT images (grayscale)\n",
    "scan_N = 9408\n",
    "oct_N = 49\n",
    "eye_N = 96\n",
    "sh = [496, 504]\n",
    "\n",
    "csv_file = '~/scratch/OLIVES/OLIVES/Biomarker_Clinical_Data_Images_Updated.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "col_names = data.columns\n",
    "file_paths = data['Path (Trial/Arm/Folder/Visit/Eye/Image Name)'].values #[9408,]\n",
    "file_paths = file_paths.reshape([eye_N,2*oct_N])\n",
    "bio_markers = data[col_names[2:18]].values\n",
    "bio_markers = bio_markers.reshape([eye_N,2*oct_N,-1])\n",
    "\n",
    "clin_data = data[col_names[19:21]].values\n",
    "clin_data = clin_data.reshape([eye_N,2*oct_N,-1])\n",
    "\n",
    "home_dir = '/home/hice1/hsuh45/scratch/OLIVES/OLIVES/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8e07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with Nan and identify the rows (get rid of them after Data split)\n",
    "rows_with_nan = data[data.isna().any(axis=1)]\n",
    "# data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43b2a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 98) (19, 98) (20, 98)\n"
     ]
    }
   ],
   "source": [
    "# DeiT preprocessing\n",
    "# transform_deit = transforms.Compose([\n",
    "#     transforms.Resize((512,512)),                # Resize to square dimensions\n",
    "#     transforms.ToTensor(),                        # Convert to tensor\n",
    "#     transforms.Normalize(mean=[0.5], std=[0.5])   # Normalize (adjust mean/std for grayscale)\n",
    "# ])\n",
    "# SwinT preprocessing\n",
    "# resize to square + fit the input size (due to small dataset)\n",
    "transform_swin = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels\n",
    "    transforms.CenterCrop(496),\n",
    "    transforms.Resize((224,224)),                # 224x224 or 384x384\n",
    "    transforms.ToTensor(),                        # Convert to tensor\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # Normalize with ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "# Create DataLoaders with the preprocessed data\n",
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = Image.open(home_dir + self.file_paths[index][0]).convert(\"L\")\n",
    "        label = self.labels[index]  # Shape: [sample N, bio_marker_N]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# Eye-wise split\n",
    "# Split dataset into train/val/test\n",
    "train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "    file_paths, bio_markers, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_val_files, train_val_labels, test_size=0.25, random_state=42\n",
    ")\n",
    "print(train_files.shape, val_files.shape, test_files.shape)\n",
    "# Eye-wise -> scan-wise\n",
    "train_files = train_files.reshape([-1,1])\n",
    "val_files = val_files.reshape([-1,1])\n",
    "test_files = test_files.reshape([-1,1])\n",
    "\n",
    "train_labels = train_labels.reshape([-1,16])\n",
    "val_labels = val_labels.reshape([-1,16])\n",
    "test_labels = test_labels.reshape([-1,16])\n",
    "\n",
    "######## Get rid of data points with Nan values #########\n",
    "train_nan = ~np.isnan(train_labels).any(axis=1)\n",
    "val_nan = ~np.isnan(val_labels).any(axis=1)\n",
    "test_nan = ~np.isnan(test_labels).any(axis=1)\n",
    "\n",
    "train_labels = train_labels[train_nan]\n",
    "val_labels = val_labels[val_nan]\n",
    "test_labels = test_labels[test_nan]\n",
    "\n",
    "train_files = train_files[train_nan]\n",
    "val_files = val_files[val_nan]\n",
    "test_files = test_files[test_nan]\n",
    "#########################################################\n",
    "\n",
    "train_dataset = OCTDataset(train_files, train_labels, transform=transform_swin)\n",
    "val_dataset = OCTDataset(val_files, val_labels, transform=transform_swin)\n",
    "test_dataset = OCTDataset(test_files, test_labels, transform=transform_swin)\n",
    "\n",
    "# Make DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e32f4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5582, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb742cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(test_labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1de52cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.81488491e-03 2.13688259e-02 2.44215154e-04 1.50192319e-02\n",
      " 2.32920203e-01 9.93345137e-02 1.98485866e-01 3.23585078e-02\n",
      " 1.13132670e-01 4.27376519e-04 1.12888455e-01 1.54343977e-01\n",
      " 8.73069174e-03 1.83161365e-04 6.10537884e-04 2.13688259e-03]\n",
      "[3.30305533e-03 2.16763006e-02 1.85796862e-03 1.42444261e-02\n",
      " 2.27497936e-01 1.20767960e-01 2.24194880e-01 2.00247729e-02\n",
      " 1.27580512e-01 0.00000000e+00 9.49628406e-02 1.32741536e-01\n",
      " 7.63831544e-03 2.06440958e-04 0.00000000e+00 3.30305533e-03]\n"
     ]
    }
   ],
   "source": [
    "# Quick analysis of biomarker distribution\n",
    "\n",
    "print(np.sum(train_labels, axis=(0)) / np.sum(train_labels))\n",
    "print(np.sum(val_labels, axis=(0)) / np.sum(val_labels))\n",
    "\n",
    "# discrepancy between train vs val class distribution (not too severe (?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd5bf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install timm\n",
    "torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4852fb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=1000, bias=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.3, inplace=False)\n",
      "  (3): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (4): Sigmoid()\n",
      ")\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:55<00:00,  1.51it/s]\n",
      " 52%|█████▏    | 30/58 [00:22<00:20,  1.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m---> 82\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[5], line 71\u001b[0m, in \u001b[0;36mvalidate_one_epoch\u001b[0;34m(model, val_loader, criterion, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 71\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m all_outputs\u001b[38;5;241m.\u001b[39mappend(outputs\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     73\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mappend(labels\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timm \n",
    "import tqdm\n",
    "\n",
    "\n",
    "# Load Swin Transformer model\n",
    "# model = timm.create_model('swin_base_patch4_window12_384', pretrained=True) \n",
    "# Load DeiT model\n",
    "model = timm.create_model('deit_base_patch16_224', pretrained=True) \n",
    "print(model.head)\n",
    "###### Parameters ######\n",
    "lr = 1e-4\n",
    "num_classes = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "# Modify the classifier head for multi-class output\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(model.head.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, num_classes),  # 16 biomarkers\n",
    "    nn.Sigmoid()  # Multi-label classification (probabilities for each class)\n",
    ")\n",
    "\n",
    "print(model.head)\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Freeze Vision Encoder layers if needed\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "criterion = torch.nn.BCELoss()  # Binary Cross-Entropy Loss for multi-label classification\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Training and validation\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader):\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "#         print(images.shape, labels.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "#         print(outputs.shape, labels.shape, images.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm.tqdm(val_loader):\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            all_outputs.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    return running_loss / len(val_loader)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate_one_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    threshold = 0.5\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(images)\n",
    "            predictions = (outputs > threshold).float()  # Threshold at 0.5 for binary decisions\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f66e4c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished iteration\n",
      "[0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "0.0 0.5\n",
      "0.0 0.5\n",
      "0.0 0.5\n",
      "0.08571428571428572 0.5234961383661482\n",
      "0.8512679917751885 0.7004670206640651\n",
      "0.3918799646954987 0.5852806999180057\n",
      "0.6673407482305359 0.6723738922057378\n",
      "0.0213903743315508 0.5041510611735331\n",
      "0.3088512241054614 0.5859964787220338\n",
      "0.0 0.5\n",
      "0.8469860896445132 0.8742755553127461\n",
      "0.8386363636363636 0.8520721444226941\n",
      "0.31746031746031744 0.5943396226415094\n",
      "0.0 0.5\n",
      "0.0 nan\n",
      "0.0 0.5\n"
     ]
    }
   ],
   "source": [
    "# Get F1 scores and AUC \n",
    "import sklearn\n",
    "test_shape = test_labels.shape\n",
    "\n",
    "def test_with_eval_metric(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    threshold = 0.5\n",
    "    target = np.zeros(test_shape)\n",
    "    pred = np.zeros(test_shape)\n",
    "    batch_size = 32\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(images)\n",
    "            pred[batch*batch_size: (batch+1)*batch_size] = (outputs > threshold).float().cpu()  # Threshold at 0.5 for binary decisions\n",
    "            target[batch*batch_size: (batch+1)*batch_size] = labels.cpu()\n",
    "#             correct += (predictions == labels).sum().item()\n",
    "#             total += labels.numel()\n",
    "    print(\"Finished iteration\")\n",
    "    print(pred[0])\n",
    "    print(target[0])\n",
    "    for i in range(num_classes):\n",
    "        f1 = sklearn.metrics.f1_score(target[:,i], pred[:,i], zero_division =0)\n",
    "        try:\n",
    "            auc = sklearn.metrics.roc_auc_score(target[:,i],pred[:,i])\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        print(f1, auc)\n",
    "#     accuracy = correct / total\n",
    "#     print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "test_with_eval_metric(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a5af49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1960, 16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b808b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "  (1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (2): Identity()\n",
       "  (3): ReLU()\n",
       "  (4): Dropout(p=0.3, inplace=False)\n",
       "  (5): Linear(in_features=512, out_features=16, bias=True)\n",
       "  (6): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Modify the classifier head for multi-class output\n",
    "model.head = nn.Sequential(\n",
    "    timm.layers.SelectAdaptivePool2d(pool_type='avg'), #, flatten=Identity()),\n",
    "    nn.Linear(model.head.in_features, 512),\n",
    "    nn.Identity(),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, num_classes),  # 16 biomarkers\n",
    "    nn.Sigmoid()  # Multi-label classification (probabilities for each class)\n",
    ")\n",
    "model.head\n",
    "# model.head.fc = nn.Linear(model.head.in_features,512)\n",
    "# model.head.relu = nn.ReLU()\n",
    "# model.head.drop2 = nn.Dropout(0.3)\n",
    "# model.head.fc2 = nn.Linear(512,16)\n",
    "# model.head.sig = nn.Sigmoid()\n",
    "# model.head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
