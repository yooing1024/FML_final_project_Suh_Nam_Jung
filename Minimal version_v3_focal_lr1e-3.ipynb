{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da45a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0362898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Load and split\n",
    "\n",
    "# 96 eyes, 49 OCT images, 2 visits, 16 biomarkers (binary)\n",
    "# 96 eyes, 49 OCT images, 2 visits, 496 x 504 OCT images (grayscale)\n",
    "scan_N = 9408\n",
    "oct_N = 49\n",
    "eye_N = 96\n",
    "sh = [496, 504]\n",
    "\n",
    "csv_file = '~/scratch/OLIVES/OLIVES/Biomarker_Clinical_Data_Images_Updated.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "col_names = data.columns\n",
    "file_paths = data['Path (Trial/Arm/Folder/Visit/Eye/Image Name)'].values #[9408,]\n",
    "file_paths = file_paths.reshape([eye_N,2*oct_N])\n",
    "bio_markers = data[col_names[2:18]].values\n",
    "bio_markers = bio_markers.reshape([eye_N,2*oct_N,-1])\n",
    "\n",
    "clin_data = data[col_names[19:21]].values\n",
    "clin_data = clin_data.reshape([eye_N,2*oct_N,-1])\n",
    "\n",
    "home_dir = '/home/hice1/hsuh45/scratch/OLIVES/OLIVES/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8e07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with Nan and identify the rows (get rid of them after Data split)\n",
    "rows_with_nan = data[data.isna().any(axis=1)]\n",
    "# data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43b2a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeiT preprocessing\n",
    "\n",
    "transform_deit = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels\n",
    "    transforms.CenterCrop(496),\n",
    "    transforms.Resize((384,384)),                # 224x224 or 384x384\n",
    "    transforms.ToTensor(),                        # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # Normalize with ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Create DataLoaders with the preprocessed data\n",
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = Image.open(home_dir + self.file_paths[index][0]).convert(\"L\")\n",
    "        label = self.labels[index]  # Shape: [sample N, bio_marker_N]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        return img, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f87118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 98) (19, 98) (20, 98)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data setup - No Stratification (OCT only)\n",
    "\n",
    "'''\n",
    "\n",
    "# Eye-wise split\n",
    "# Split dataset into train/val/test\n",
    "train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "    file_paths, bio_markers, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_val_files, train_val_labels, test_size=0.25, random_state=42\n",
    ")\n",
    "print(train_files.shape, val_files.shape, test_files.shape)\n",
    "# Eye-wise -> scan-wise\n",
    "train_files = train_files.reshape([-1,1])\n",
    "val_files = val_files.reshape([-1,1])\n",
    "test_files = test_files.reshape([-1,1])\n",
    "\n",
    "train_labels = train_labels.reshape([-1,16])\n",
    "val_labels = val_labels.reshape([-1,16])\n",
    "test_labels = test_labels.reshape([-1,16])\n",
    "\n",
    "######## Get rid of data points with Nan values #########\n",
    "train_nan = ~np.isnan(train_labels).any(axis=1)\n",
    "val_nan = ~np.isnan(val_labels).any(axis=1)\n",
    "test_nan = ~np.isnan(test_labels).any(axis=1)\n",
    "\n",
    "train_labels = train_labels[train_nan]\n",
    "val_labels = val_labels[val_nan]\n",
    "test_labels = test_labels[test_nan]\n",
    "\n",
    "train_files = train_files[train_nan]\n",
    "val_files = val_files[val_nan]\n",
    "test_files = test_files[test_nan]\n",
    "#########################################################\n",
    "\n",
    "train_dataset = OCTDataset(train_files, train_labels, transform=transform_deit)\n",
    "val_dataset = OCTDataset(val_files, val_labels, transform=transform_deit)\n",
    "test_dataset = OCTDataset(test_files, test_labels, transform=transform_deit)\n",
    "\n",
    "# Make DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f3afbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "(57, 98) (19, 98) (20, 98)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dataset setup with Stratification (OCT only)\n",
    "'''\n",
    "\n",
    "# Eye-wise split\n",
    "# Split dataset into train/val/test\n",
    "\n",
    "####### Handle rows with Nan -> mark them with ones(16,) ############\n",
    "nan_rows = np.isnan(bio_markers).any(axis=2)  # Shape (96, 98), True where NaNs are present\n",
    "print(np.sum(nan_rows))\n",
    "# Replace NaN rows ones (checked that no data rows are filled with 1's)\n",
    "alternating_row = np.ones(16)  \n",
    "bio_markers[nan_rows] = alternating_row\n",
    "\n",
    "####### Data stratification based on positive label count per eye #######\n",
    "pos_count = np.sum(bio_markers,axis=(1,2)) # pos label count per eye\n",
    "bins = [0,200,300,400,np.inf]\n",
    "# print(pos_count)\n",
    "stratify_bins = np.digitize(pos_count, bins)\n",
    "# print(stratify_bins)\n",
    "train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "    file_paths, bio_markers, test_size=0.2,\n",
    "    stratify = stratify_bins,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pos_count = np.sum(train_val_labels,axis=(1,2)) # pos label count per eye\n",
    "bins = [0,200,300,400,np.inf]\n",
    "stratify_bins = np.digitize(pos_count, bins)\n",
    "# print(pos_count)\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "    train_val_files, train_val_labels, test_size=0.25, \n",
    "    stratify = stratify_bins,\n",
    "    random_state=42\n",
    ")\n",
    "print(train_files.shape, val_files.shape, test_files.shape)\n",
    "##########################################################\n",
    "# Eye-wise -> scan-wise\n",
    "train_files = train_files.reshape([-1,1])\n",
    "val_files = val_files.reshape([-1,1])\n",
    "test_files = test_files.reshape([-1,1])\n",
    "\n",
    "train_labels = train_labels.reshape([-1,16])\n",
    "val_labels = val_labels.reshape([-1,16])\n",
    "test_labels = test_labels.reshape([-1,16])\n",
    "\n",
    "############################################\n",
    "# Get rid of marked Nan rows\n",
    "train_nan = np.sum(train_labels,axis=1)!=16\n",
    "val_nan = np.sum(val_labels,axis=1)!=16\n",
    "test_nan = np.sum(test_labels,axis=1)!=16\n",
    "\n",
    "train_labels = train_labels[train_nan]\n",
    "val_labels = val_labels[val_nan]\n",
    "test_labels = test_labels[test_nan]\n",
    "\n",
    "train_files = train_files[train_nan]\n",
    "val_files = val_files[val_nan]\n",
    "test_files = test_files[test_nan]\n",
    "\n",
    "########################################################\n",
    "\n",
    "train_dataset = OCTDataset(train_files, train_labels, transform=transform_deit)\n",
    "val_dataset = OCTDataset(val_files, val_labels, transform=transform_deit)\n",
    "test_dataset = OCTDataset(test_files, test_labels, transform=transform_deit)\n",
    "\n",
    "# Make DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3a554366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 178.,  616.,   44.,  385., 6350., 2996., 5228.,  819., 2848.,\n",
       "         22., 3015., 4099.,  245.,   22.,   22.,   88.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check distribution of bio markers\n",
    "print(np.sum(bio_markers,axis=(0,1)))\n",
    "# Marker 2,9,13,14,15 has <100 markers -> Stratification based on these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d82315",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset setup with Stratification for low-sample markers (OCT only)\n",
    "'''\n",
    "\n",
    "# TODO : \n",
    "# Here, stratify_labels indicates whether a sample contains positive instances for any rare biomarker (1 for positive, 0 otherwise).\n",
    "\n",
    "\n",
    "# stratify_labels = np.any(bio_markers[:, rare_biomarkers] > 0, axis=1).astype(int)\n",
    "# train_val_files, test_files, train_val_labels, test_labels = train_test_split(\n",
    "#     file_paths, bio_markers, test_size=0.2, stratify=stratify_labels, random_state=42\n",
    "# )\n",
    "# train_val_stratify_labels = np.any(train_val_labels[:, rare_biomarkers] > 0, axis=1).astype(int)\n",
    "\n",
    "# train_files, val_files, train_labels, val_labels = train_test_split(\n",
    "#     train_val_files, train_val_labels, test_size=0.25, stratify=train_val_stratify_labels, random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa184914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9156cba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 3), dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.isnan(clin_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1de52cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.36595641 0.98581517 0.13144202 1.05439362 1.02383436 0.82252373\n",
      " 0.88532738 1.61592384 0.8867551         inf 1.18876451 1.16274063\n",
      " 1.14301272 0.88723365        inf 0.64694121]\n",
      "[ 0.50613819  1.25364514 13.98781913  0.69430339  1.09687196  1.3972887\n",
      "  0.79621124  1.00012212  0.58006124  1.26205887  1.10529691  1.06702551\n",
      "  1.09143086  5.88960805  0.          2.10343145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/995974/ipykernel_1802866/3005845574.py:6: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  print(train_ratio/val_ratio)\n"
     ]
    }
   ],
   "source": [
    "# Quick analysis of biomarker distribution\n",
    "\n",
    "train_ratio = np.sum(train_labels, axis=(0)) / np.sum(train_labels)\n",
    "val_ratio = np.sum(val_labels, axis=(0)) / np.sum(val_labels)\n",
    "test_ratio = np.sum(test_labels, axis=(0)) / np.sum(test_labels)\n",
    "print(train_ratio/val_ratio)\n",
    "print(test_ratio/train_ratio)\n",
    "\n",
    "# discrepancy between train vs val class distribution (not too severe (?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c78ad0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.740e+02, 5.780e+02, 5.800e+01, 2.620e+02, 7.258e+03, 3.390e+03,\n",
       "        6.770e+03, 1.078e+03, 3.914e+03, 0.000e+00, 3.376e+03, 4.634e+03,\n",
       "        2.340e+02, 4.000e+00, 2.000e+01, 1.020e+02]),\n",
       " array([  32.,  408.,    6.,  210., 2532., 1328., 1712.,  316.,  926.,\n",
       "          14., 1222., 1740.,  136.,   14.,    0.,    8.]),\n",
       " array([2.600e+01, 2.220e+02, 0.000e+00, 2.740e+02, 2.886e+03, 1.250e+03,\n",
       "        1.950e+03, 2.200e+02, 8.320e+02, 6.000e+00, 1.408e+03, 1.800e+03,\n",
       "        9.600e+01, 2.000e+00, 0.000e+00, 4.200e+01]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2* np.sum(train_labels,axis=0)),(2* np.sum(val_labels,axis=0)),(2* np.sum(test_labels,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "568dc6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted BCE for multi-label imbalanced (pos vs. neg) data\n",
    "\n",
    "train_pos_weights = torch.tensor(train_labels.shape[0] / (2* np.sum(train_labels,axis=0)))\n",
    "# val_pos_weights = val_labels.shape[0] / (2* np.sum(val_labels,axis=0))\n",
    "# test_pos_weights = test_labels.shape[0] / (2* np.sum(test_labels,axis=0))\n",
    "\n",
    "class WeightedBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, pos_weights):\n",
    "        \"\"\"\n",
    "        pos_weights: Tensor of shape (num_biomarkers,) containing weights for positive labels.\n",
    "        \"\"\"\n",
    "        super(WeightedBinaryCrossEntropyLoss, self).__init__()\n",
    "        self.pos_weights = pos_weights\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        logits: Predicted logits from the model, shape (batch_size, num_biomarkers).\n",
    "        targets: Ground truth binary labels, shape (batch_size, num_biomarkers).\n",
    "        \"\"\"\n",
    "        loss = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)  # Compute BCE loss\n",
    "        weighted_loss = loss * self.pos_weights  # Apply positive weights\n",
    "        return weighted_loss.mean()\n",
    "    \n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss for multi-label classification.\n",
    "\n",
    "        Parameters:\n",
    "        - gamma (float): Focusing parameter that reduces the loss for well-classified samples (default: 2.0).\n",
    "        - alpha (float or Tensor): Balancing factor to address class imbalance (default: None).\n",
    "          If a tensor is provided, it should be of shape (num_classes,).\n",
    "        - reduction (str): Specifies the reduction to apply to the output: 'none', 'mean', 'sum' (default: 'mean').\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Compute Focal Loss.\n",
    "\n",
    "        Parameters:\n",
    "        - logits (Tensor): Predicted logits of shape (batch_size, num_classes).\n",
    "        - targets (Tensor): Ground truth labels of shape (batch_size, num_classes).\n",
    "\n",
    "        Returns:\n",
    "        - loss (Tensor): Calculated focal loss.\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # Binary cross-entropy loss\n",
    "        bce_loss = F.binary_cross_entropy(probs, targets, reduction='none')\n",
    "        \n",
    "        # Compute the modulating factor (1 - p_t)^gamma\n",
    "        pt = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_factor = (1 - pt) ** self.gamma\n",
    "\n",
    "        # Apply class balancing factor alpha if provided\n",
    "        if self.alpha is not None:\n",
    "            if isinstance(self.alpha, (float, int)):\n",
    "                alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            elif isinstance(self.alpha, torch.Tensor):\n",
    "                alpha_factor = self.alpha.unsqueeze(0) * targets + (1 - self.alpha).unsqueeze(0) * (1 - targets)\n",
    "            else:\n",
    "                raise ValueError(\"Alpha must be a float, int, or torch.Tensor.\")\n",
    "            focal_loss = alpha_factor * focal_factor * bce_loss\n",
    "        else:\n",
    "            focal_loss = focal_factor * bce_loss\n",
    "\n",
    "        # Reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Initialize Focal Loss (Example)\n",
    "# 1. gamma (Focusing Parameter):\n",
    "# Controls the strength of the focusing effect.\n",
    "# Higher values put more focus on hard-to-classify samples.\n",
    "\n",
    "# 2. alpha (Class Balancing Factor):\n",
    "# Helps address class imbalance.\n",
    "# If alpha is a scalar, it applies the same balancing for all classes.\n",
    "# If alpha is a tensor, it applies per-class balancing.\n",
    "\n",
    "# 3. reduction:\n",
    "# 'mean': Average loss across the batch.\n",
    "# 'sum': Sum of the loss across the batch.\n",
    "# 'none': No reduction is applied; returns loss for each sample.\n",
    "\n",
    "focal_loss = FocalLoss(gamma=2.0, alpha=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4852fb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name deit_base_patch16_224, input size 224\n",
      "Linear(in_features=768, out_features=1000, bias=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (4): Sigmoid()\n",
      ")\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:33<00:00,  1.87it/s]\n",
      "100%|██████████| 59/59 [00:37<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1157, Val Loss: 0.1156\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:32<00:00,  1.88it/s]\n",
      "100%|██████████| 59/59 [00:46<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1140, Val Loss: 0.1156\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:08<00:00,  2.54it/s]\n",
      "100%|██████████| 59/59 [00:43<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1138, Val Loss: 0.1155\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:45<00:00,  1.66it/s]\n",
      "100%|██████████| 59/59 [00:39<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1136, Val Loss: 0.1154\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:42<00:00,  1.71it/s]\n",
      "100%|██████████| 59/59 [00:49<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1135, Val Loss: 0.1152\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:54<00:00,  1.53it/s]\n",
      "100%|██████████| 59/59 [00:26<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1132, Val Loss: 0.1152\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:54<00:00,  1.53it/s]\n",
      "100%|██████████| 59/59 [00:24<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1131, Val Loss: 0.1153\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:47<00:00,  1.63it/s]\n",
      "100%|██████████| 59/59 [00:24<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1131, Val Loss: 0.1152\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:46<00:00,  1.65it/s]\n",
      "100%|██████████| 59/59 [00:55<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1130, Val Loss: 0.1152\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [02:18<00:00,  1.27it/s]\n",
      "100%|██████████| 59/59 [01:02<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1130, Val Loss: 0.1153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import timm \n",
    "import tqdm\n",
    "\n",
    "# Load model (model_name, )\n",
    "model_name = 'deit_base_patch16_384'\n",
    "input_dim = 384\n",
    "print(f'model name {model_name}, input size {input_dim}')\n",
    "model = timm.create_model(model_name, pretrained=True) \n",
    "print(model.head)\n",
    "###### Parameters ######\n",
    "lr = 1e-3\n",
    "num_classes = 16\n",
    "epochs = 10\n",
    "########################\n",
    "\n",
    "# Modify the classifier head for multi-class output\n",
    "model.head = nn.Sequential(\n",
    "    nn.Linear(model.head.in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, num_classes),  # 16 biomarkers\n",
    "    nn.Sigmoid()  # Multi-label classification (probabilities for each class)\n",
    ")\n",
    "\n",
    "print(model.head)\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Freeze Vision Encoder layers if needed\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "######### Loss ###########\n",
    "# criterion = torch.nn.BCELoss()  # Binary Cross-Entropy Loss for multi-label classification\n",
    "criterion = WeightedBinaryCrossEntropyLoss(train_pos_weights.to('cuda'))\n",
    "\n",
    "# train_pos_weights = torch.tensor(1/ (np.sum(train_labels,axis=0)))\n",
    "# criterion = FocalLoss(gamma=2.0, alpha=train_pos_weights.to('cuda'))\n",
    "##########################\n",
    "\n",
    "# Training and validation\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm.tqdm(train_loader):\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "#         print('loss : ',loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = np.zeros([len(val_dataset),num_classes])\n",
    "    all_labels = np.zeros([len(val_dataset),num_classes])\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm.tqdm(val_loader):\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "#             print(i*batch_size, min((i+1)*batch_size,len(val_dataset)))\n",
    "#             print(outputs.shape)\n",
    "            all_outputs[i*batch_size: min((i+1)*batch_size,len(val_dataset)),:] = outputs.cpu()\n",
    "            all_labels[i*batch_size: min((i+1)*batch_size,len(val_dataset)),:] = labels.cpu()\n",
    "            i+=1\n",
    "\n",
    "    return running_loss / len(val_loader), all_outputs, all_labels\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_outputs, val_labels = validate_one_epoch(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    scheduler.step()\n",
    "\n",
    "# # Test the model\n",
    "# def test_model(model, test_loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     threshold = 0.5\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to('cuda'), labels.to('cuda')\n",
    "#             outputs = model(images)\n",
    "#             predictions = (outputs > threshold).float()  # Threshold at 0.5 for binary decisions\n",
    "#             correct += (predictions == labels).sum().item()\n",
    "#             total += labels.numel()\n",
    "\n",
    "#     accuracy = correct / total\n",
    "#     print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Evaluate on test set\n",
    "\n",
    "# test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e9c8996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1854"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1a2bae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 threshold [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010101010101010102, 0.0, 0.0, 0.0, 0.0], F1 validation scores [0.02545068928950159, 0.11443037974683544, 0.018094731240021287, 0.07942238267148015, 0.746971736204576, 0.48309572301425663, 0.7401894451962111, 0.10676156583629894, 0.5032154340836013, 0.008556149732620321, 0.40171673819742487, 0.6725043782837128, 0.04719454640797063, 0.009620523784072688, 0.008556149732620321, 0.02545068928950159]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "'''\n",
    "Optimize biomarker-wise threshold for validation on eval metrics\n",
    "\n",
    "'''\n",
    "\n",
    "def optimal_thresholds(model_outputs, labels, metric='f1'):\n",
    "    \"\"\"\n",
    "    Calculate optimal thresholds for each biomarker.\n",
    "    \n",
    "    Parameters:\n",
    "        model_outputs (ndarray): Model predictions, shape (N, 16) where N is the number of samples.\n",
    "        labels (ndarray): True binary labels, shape (N, 16).\n",
    "        metric (str): Metric to optimize. Options: 'f1', 'auc'.\n",
    "\n",
    "    Returns:\n",
    "        thresholds (list): Optimal threshold for each biomarker.\n",
    "        scores (list): Corresponding best scores for each biomarker.\n",
    "    \"\"\"\n",
    "    num_biomarkers = model_outputs.shape[1]\n",
    "    thresholds = []\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(num_biomarkers):\n",
    "        best_threshold = 0.0\n",
    "        best_score = 0.0\n",
    "        \n",
    "        # Thresholds to search\n",
    "        thresholds_range = np.linspace(0, 1, 100)\n",
    "        \n",
    "        for threshold in thresholds_range:\n",
    "            preds = (model_outputs[:, i] >= threshold).astype(int)\n",
    "            \n",
    "            if metric == 'f1':\n",
    "                score = sklearn.metrics.f1_score(labels[:, i], preds)\n",
    "            elif metric == 'auc':\n",
    "                try:\n",
    "                    score = sklearn.metrics.roc_auc_score(labels[:,i],model_outputs[:,i])\n",
    "                except ValueError:\n",
    "                    score = np.nan\n",
    "#                 # AUC does not depend on a threshold\n",
    "#                 score = sklearn.metrics.roc_auc_score(labels[:, i], model_outputs[:, i])\n",
    "#                 best_threshold = None  # No threshold needed for AUC\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(\"Unsupported metric. Use 'f1' or 'auc'.\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        thresholds.append(best_threshold)\n",
    "        scores.append(best_score)\n",
    "    \n",
    "    return thresholds, scores\n",
    "\n",
    "f1_th, f1_scores = optimal_thresholds(val_outputs, val_labels, metric='f1')\n",
    "# auc_th, auc_scores = optimal_thresholds(val_outputs, val_labels, metric='auc')\n",
    "print(f'F1 threshold {f1_th}, F1 validation scores {f1_scores}')\n",
    "# print(f'AUC threshold {auc_th}, AUC validation scores {auc_scores}')\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f66e4c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02518891687657431\n",
      "Metrics saved to deit284_focal_lr_f1_threshold_metrics.json\n",
      "0.0\n",
      "Metrics saved to deit284_focal_lr_0.5threshold_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Get F1 scores and AUC \n",
    "import sklearn\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import timm\n",
    "import tqdm\n",
    "from torch import nn\n",
    "test_shape = test_labels.shape\n",
    " \n",
    "def test_with_eval_metric(model, test_loader, threshold, batch_size, output_path):\n",
    "    \"\"\"\n",
    "    Evaluate the model using test data and save F1, AUC scores, and thresholds as JSON.\n",
    " \n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.\n",
    "        threshold (list): List of thresholds for classification.\n",
    "        batch_size (int): Batch size.\n",
    "        output_path (str): Path to save the JSON metrics file.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_shape = (len(test_loader.dataset), len(threshold))\n",
    "    target = np.zeros(test_shape)\n",
    "    pred = np.zeros(test_shape)\n",
    " \n",
    "    threshold_tensor = torch.tensor(threshold).to(\"cuda\")\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for batch, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = model(images)\n",
    "            pred[\n",
    "                batch * batch_size : min((batch + 1) * batch_size, len(test_loader.dataset))\n",
    "            ] = (outputs > threshold_tensor).float().cpu()\n",
    "            target[\n",
    "                batch * batch_size : min((batch + 1) * batch_size, len(test_loader.dataset))\n",
    "            ] = labels.cpu()\n",
    " \n",
    "    metrics = {}\n",
    "    for i in range(len(threshold)):\n",
    "        f1 = sklearn.metrics.f1_score(target[:, i], pred[:, i], zero_division=0)\n",
    "        try:\n",
    "            auc = sklearn.metrics.roc_auc_score(target[:, i], pred[:, i])\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        metrics[f\"Biomarker_{i}\"] = {\n",
    "            \"F1\": f1,\n",
    "            \"AUC\": auc,\n",
    "            \"Threshold\": threshold[i]\n",
    "        }\n",
    " \n",
    "    # Save to JSON\n",
    "    with open(output_path, \"w\") as json_file:\n",
    "        json.dump(metrics, json_file, indent=4)\n",
    "    print(f1)\n",
    "    print(f\"Metrics saved to {output_path}\")\n",
    " \n",
    " \n",
    "# Eval test with optimized F1 Threshold\n",
    "opt_output_path = \"deit384_focal_lr_f1_threshold_metrics.json\"\n",
    "default_output_path = \"deit384_focal_lr_0.5threshold_metrics.json\"\n",
    "test_with_eval_metric(model, test_loader, threshold=f1_th, batch_size=batch_size, output_path=opt_output_path)\n",
    "test_with_eval_metric(model, test_loader, threshold=[0.5]*16, batch_size=batch_size, output_path=default_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be0ba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to prelim_deit224_stratO_Focal_metrics.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import timm\n",
    "import tqdm\n",
    "from torch import nn\n",
    " \n",
    "# Function to save metrics as JSON\n",
    "def save_metrics_to_json(file_path, metrics):\n",
    "    \"\"\"\n",
    "    Save metrics to a JSON file.\n",
    "    Parameters:\n",
    "        file_path (str): Path to save the JSON file.\n",
    "        metrics (dict): Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(metrics, json_file, indent=4)\n",
    "        \n",
    "    return\n",
    " \n",
    "# Evaluation function with JSON output\n",
    "def test_with_eval_metric_json(model, test_loader, threshold, batch_size, num_classes, output_path):\n",
    "    \"\"\"\n",
    "    Evaluate the model using test data and save F1 and AUC scores as JSON.\n",
    " \n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test dataset.\n",
    "        threshold (list): List of thresholds for classification.\n",
    "        batch_size (int): Batch size.\n",
    "        num_classes (int): Number of output classes.\n",
    "        output_path (str): Path to save the JSON metrics file.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_shape = (len(test_loader.dataset), num_classes)\n",
    "    target = np.zeros(test_shape)\n",
    "    pred = np.zeros(test_shape)\n",
    "    threshold = torch.tensor(threshold).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        for batch, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(images)\n",
    "            pred[batch*batch_size: min((batch+1)*batch_size, len(test_loader.dataset))] = (outputs > threshold).float().cpu()\n",
    "            target[batch*batch_size: min((batch+1)*batch_size, len(test_loader.dataset))] = labels.cpu()\n",
    " \n",
    "    metrics = {}\n",
    "    for i in range(num_classes):\n",
    "        f1 = sklearn.metrics.f1_score(target[:, i], pred[:, i], zero_division=0)\n",
    "        try:\n",
    "            auc = sklearn.metrics.roc_auc_score(target[:, i], pred[:, i])\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        metrics[f\"Biomarker_{i}\"] = {\"F1\": f1, \"AUC\": auc}\n",
    " \n",
    "    # Save to JSON\n",
    "    save_metrics_to_json(output_path, metrics)\n",
    "    print(f\"Metrics saved to {output_path}\")\n",
    "    return\n",
    " \n",
    "# Example usage for VGG16\n",
    "# output_path = \"prelim_deit224_stratO_BCE_metrics.json\"\n",
    "# output_path = \"prelim_deit224_stratO_Weighted_metrics.json\"\n",
    "output_path = \"prelim_deit384_metrics.json\"\n",
    "test_with_eval_metric_json(\n",
    "    model, \n",
    "    test_loader, \n",
    "    threshold=f1_th, \n",
    "    batch_size=batch_size, \n",
    "    num_classes=num_classes, \n",
    "    output_path=output_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
